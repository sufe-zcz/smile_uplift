{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklift.metrics import qini_auc_score, uplift_auc_score\n",
    "from causalml.dataset import make_uplift_classification_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleComparisonNet(nn.Module):\n",
    "    def __init__(self, input_dim, shared_hidden=128, embedding_dim=64):\n",
    "        super(SampleComparisonNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=shared_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=shared_hidden, out_features=shared_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=shared_hidden, out_features=embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=shared_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=shared_hidden, out_features=shared_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=shared_hidden, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_embeddings(self, inputs):\n",
    "        # Return embeddings. No gradient needed.\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.encoder(inputs)\n",
    "        return embeddings\n",
    "\n",
    "    def compute_similarity(self, embeddings, tags, top_k=5):\n",
    "        # For each sample, get topK indices of other-group samples by cosine similarity.\n",
    "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        cosine_similarity = torch.matmul(normalized_embeddings, normalized_embeddings.T)\n",
    "        tag = tags.float().view(-1, 1)\n",
    "        mask = (tag != tag.t())\n",
    "        masked_similarity = cosine_similarity.masked_fill(~mask, float('-inf'))\n",
    "        _, topk_indices = torch.topk(masked_similarity, k=top_k, dim=1)\n",
    "        return topk_indices\n",
    "\n",
    "    def generate_comparison_labels(self, labels, topk_indices, tags, top_k=5):\n",
    "        # For each sample, generate uplift label by group.\n",
    "        label_vector = torch.squeeze(labels, dim=1)\n",
    "        tags_vector = torch.squeeze(tags, dim=1)\n",
    "        topk_labels = label_vector[topk_indices]              # [N, top_k]\n",
    "        matched_y = topk_labels.float().mean(dim=1)           # [N]\n",
    "        uplift_label = torch.where(\n",
    "            tags_vector > 0,\n",
    "            label_vector - matched_y,                         # Treatment: Y(1) - matched_Y(0)\n",
    "            matched_y - label_vector                          # Control: matched_Y(1) - Y(0)\n",
    "        )\n",
    "        return uplift_label.unsqueeze(1)\n",
    "\n",
    "    def compute_loss(self, predictions, comparison_labels, tags):\n",
    "        assert predictions.shape == comparison_labels.shape\n",
    "        loss = F.mse_loss(predictions, comparison_labels, reduction='mean')\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    # Set all random seeds for reproducibility.\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleComparisonNetModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        shared_hidden=64,\n",
    "        embedding_dim=16,\n",
    "        epochs=5,\n",
    "        patience=1,\n",
    "        batch_size=2048,\n",
    "        learning_rate=1e-4,\n",
    "        data_loader_num_workers=10,\n",
    "        random_seed=30,\n",
    "        scheduler_patience=2,\n",
    "        scheduler_factor=0.5,\n",
    "        scheduler_min_lr=1e-7,\n",
    "        top_k=5,\n",
    "    ):\n",
    "        set_seed(random_seed)\n",
    "        self.model = SampleComparisonNet(input_dim, shared_hidden, embedding_dim).to(device)\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = data_loader_num_workers\n",
    "        self.top_k = top_k\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "        self.train_dataloader = None\n",
    "        self.valid_dataloader = None\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optim,\n",
    "            mode='min',\n",
    "            patience=scheduler_patience,\n",
    "            factor=scheduler_factor,\n",
    "            min_lr=scheduler_min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def create_dataloaders(self, x, y, tags, valid_perc=None):\n",
    "        # Split/train-validation loader.\n",
    "        if valid_perc:\n",
    "            x_train, x_test, y_train, y_test, tags_train, tags_test = train_test_split(\n",
    "                x, y, tags, test_size=valid_perc, random_state=42\n",
    "            )\n",
    "            x_train = torch.Tensor(x_train)\n",
    "            x_test = torch.Tensor(x_test)\n",
    "            y_train = torch.Tensor(y_train).reshape(-1, 1)\n",
    "            y_test = torch.Tensor(y_test).reshape(-1, 1)\n",
    "            tags_train = torch.Tensor(tags_train).reshape(-1, 1)\n",
    "            tags_test = torch.Tensor(tags_test).reshape(-1, 1)\n",
    "            train_dataset = TensorDataset(x_train, y_train, tags_train)\n",
    "            valid_dataset = TensorDataset(x_test, y_test, tags_test)\n",
    "            self.train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "            self.valid_dataloader = DataLoader(valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "        else:\n",
    "            x = torch.Tensor(x)\n",
    "            y = torch.Tensor(y).reshape(-1, 1)\n",
    "            tags = torch.Tensor(tags).reshape(-1, 1)\n",
    "            train_dataset = TensorDataset(x, y, tags)\n",
    "            self.train_dataloader = DataLoader(\n",
    "                train_dataset, batch_size=self.batch_size, num_workers=self.num_workers\n",
    "            )\n",
    "\n",
    "    def fit(self, x, y, tags, valid_perc=None):\n",
    "        self.create_dataloaders(x, y, tags, valid_perc)\n",
    "        early_stopper = EarlyStopper(patience=self.patience, min_delta=0)\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss_sum = 0.0\n",
    "            num_batches = 0\n",
    "            for batch, (X, y_batch, tags_batch) in enumerate(self.train_dataloader):\n",
    "                self.model.train()\n",
    "                X, y_batch, tags_batch = X.to(device), y_batch.to(device), tags_batch.to(device)\n",
    "                embeddings = self.model.get_embeddings(X)\n",
    "                topk_indices = self.model.compute_similarity(embeddings, tags_batch, top_k=self.top_k)\n",
    "                comparison_labels = self.model.generate_comparison_labels(y_batch, topk_indices, tags_batch, top_k=self.top_k)\n",
    "                predictions = self.model(X)\n",
    "                loss = self.model.compute_loss(predictions, comparison_labels, tags_batch)\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                train_loss_sum += loss.item()\n",
    "                num_batches += 1\n",
    "            train_loss_mean = train_loss_sum / num_batches\n",
    "\n",
    "            if self.valid_dataloader:\n",
    "                self.model.eval()\n",
    "                valid_loss = self.validate_step()\n",
    "                print(f\"epoch: {epoch} | train_loss: {train_loss_mean:.8f} | valid_loss: {valid_loss:.8f} | lr: {self.optim.param_groups[0]['lr']:.2e}\")\n",
    "                self.scheduler.step(valid_loss)\n",
    "                if early_stopper.early_stop(valid_loss):\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"epoch: {epoch} | train_loss: {train_loss_mean:.8f} | lr: {self.optim.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    def validate_step(self):\n",
    "        valid_loss_sum = 0.0\n",
    "        num_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y_batch, tags_batch) in enumerate(self.valid_dataloader):\n",
    "                embeddings = self.model.get_embeddings(X.to(device))\n",
    "                topk_indices = self.model.compute_similarity(embeddings, tags_batch.to(device), top_k=self.top_k)\n",
    "                comparison_labels = self.model.generate_comparison_labels(y_batch.to(device), topk_indices, tags_batch.to(device), top_k=self.top_k)\n",
    "                predictions = self.model(X.to(device))\n",
    "                loss = self.model.compute_loss(predictions, comparison_labels, tags_batch.to(device))\n",
    "                valid_loss_sum += loss.item()\n",
    "                num_batches += 1\n",
    "        return valid_loss_sum / num_batches\n",
    "\n",
    "    def predict(self, x, batch_size=1024):\n",
    "        # Batch prediction on test set.\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(x), batch_size):\n",
    "                x_batch = torch.Tensor(x[i:i+batch_size]).to(device)\n",
    "                pred = self.model(x_batch)\n",
    "                preds.append(pred.cpu())\n",
    "        return torch.cat(preds, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Synthetic Data Generation ====\n",
    "df, x_names = make_uplift_classification_logistic(\n",
    "    n_samples=50000,\n",
    "    treatment_name=[\"control\", \"treatment\"],\n",
    "    y_name=\"outcome\",\n",
    "    n_classification_features=150,\n",
    "    n_classification_informative=10,\n",
    "    n_classification_redundant=70,\n",
    "    n_classification_repeated=50,\n",
    "    n_uplift_dict={\"treatment\": 80},\n",
    "    n_mix_informative_uplift_dict={\"treatment\": 70},\n",
    "    delta_uplift_dict={\"treatment\": 0.1},\n",
    "    positive_class_proportion=0.05,\n",
    "    random_seed=47,\n",
    "    feature_association_list=['sin', 'cos', 'cubic', 'relu']*38\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['treatment'] = (df['treatment_group_key'] == 'treatment').astype(int)\n",
    "df['true_uplift'] = df['treatment_true_effect']\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "in_features = x_names\n",
    "label_feature = 'outcome'\n",
    "treatment_feature = 'treatment'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[in_features])\n",
    "X_test = scaler.transform(test_df[in_features])\n",
    "y_train = train_df[label_feature].values\n",
    "y_test = test_df[label_feature].values\n",
    "tags_train = train_df[treatment_feature].values\n",
    "tags_test = test_df[treatment_feature].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Model Training & Evaluation ====\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "model = SampleComparisonNetModel(\n",
    "    input_dim=X_train.shape[1],\n",
    "    shared_hidden=512,\n",
    "    embedding_dim=32,\n",
    "    random_seed=seed,\n",
    "    batch_size=20000,\n",
    "    learning_rate=5e-4,\n",
    "    epochs=40,\n",
    "    patience=10,\n",
    "    scheduler_patience=5,\n",
    "    scheduler_factor=0.5,\n",
    "    scheduler_min_lr=1e-10,\n",
    "    top_k=8\n",
    ")\n",
    "model.fit(X_train, y_train, tags_train, valid_perc=0.1)\n",
    "pred = model.predict(X_test).cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "analysis_df = pd.DataFrame({\n",
    "    'treatment': test_df['treatment'].values,\n",
    "    'label': test_df['outcome'].values,\n",
    "    'uplift_pred': pred,\n",
    "    'true_uplift': test_df['true_uplift'].values\n",
    "})\n",
    "\n",
    "y_true = analysis_df['label'].values\n",
    "treatment = analysis_df['treatment'].values\n",
    "uplift_score = analysis_df['uplift_pred'].values\n",
    "true_uplift = analysis_df['true_uplift'].values\n",
    "\n",
    "auuc = uplift_auc_score(y_true, uplift_score, treatment)\n",
    "qini = qini_auc_score(y_true, uplift_score, treatment)\n",
    "sqrt_pehe = np.sqrt(np.mean((uplift_score - true_uplift) ** 2))\n",
    "pred_ate = np.mean(uplift_score)\n",
    "true_ate = np.mean(true_uplift)\n",
    "e_ate = np.abs(pred_ate - true_ate)\n",
    "print(f\"\\n==== Results ====\")\n",
    "print(f\"Seed {seed} | AUUC={auuc:.6f} | Qini={qini:.6f} | sqrt_PEHE={sqrt_pehe:.6f} | Îµ_ATE={e_ate:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
